//
//  Localizable.strings
//  OllamaSpring
//
//  Created by NeilStudio on 2025/6/17.
//

"Welcome to OllamaSpring" = "Bienvenue dans OllamaSpring";
"welcome.help_today" = "Comment puis-je vous aider aujourd'hui ?";
"welcome.no_model_message" = "Désolé, vous devez d'abord télécharger un modèle Ollama. Vous pouvez trouver un bouton de téléchargement en bas à gauche. Amusez-vous bien !";
"welcome.description" = "OllamaSpring est un client Mac complet pour gérer les différents modèles proposés par la communauté ollama et créer des expériences d'IA conversationnelle.";

// SendMsgPanel
"sendmsg.revoke" = "Révoquer";
"sendmsg.voice_not_available" = "La conversion voix-texte n'est pas disponible actuellement";
"sendmsg.deepseek_upload_coming" = "Le téléchargement de fichiers pour DeepSeek arrive bientôt";
"sendmsg.select_model_first" = "Vous devez d'abord sélectionner un modèle dans la barre supérieure ou télécharger un modèle";
"sendmsg.create_conversation_first" = "Vous devez d'abord créer une nouvelle conversation dans la barre supérieure gauche.";

// MainPanel
"main.welcome" = "Bienvenue dans OllamaSpring";
"main.start_without_ollama" = "Démarrer sans Ollama";
"main.ollama_not_available" = "Le service API Ollama n'est pas disponible sur votre Mac. Si vous souhaitez exécuter des modèles Ollama localement sur votre Mac, suivez ces étapes pour installer et configurer Ollama en premier. Si vous hébergez le service API Ollama sur un hôte spécifique, vous devez simplement entrer votre propre hôte Ollama ci-dessous.";
"main.step1_install" = "Étape 1 : Installer Ollama";
"main.step2_refresh" = "Étape 2 : Actualiser";
"main.enter_ollama_host" = "Entrez votre propre hôte Ollama";

// MessagesPanel
"messages.assistant" = "Assistant";
"messages.text" = "Texte";
"messages.waiting" = "En attente...";

// ChatListPanel
"chatlist.conversation" = "Conversation";
"chatlist.model" = "Modèle";
"chatlist.download_first" = "Vous devez d'abord télécharger un modèle et sélectionner un modèle préféré avant de créer un nouveau chat";
"chatlist.remove" = "Supprimer";
"chatlist.downloads" = "Téléchargements";
"chatlist.reset_all" = "Tout réinitialiser";
"chatlist.temperature" = "Température";
"chatlist.temperature_desc" = "La température du modèle. Augmenter la température rendra les réponses du modèle plus créatives. (Par défaut : 0.8)";
"chatlist.seed" = "Graine";
"chatlist.seed_desc" = "Définit la graine de nombres aléatoires à utiliser pour la génération. Définir ceci à un nombre spécifique fera que le modèle générera le même texte pour le même prompt. (Par défaut : 0)";
"chatlist.context_tokens" = "Jetons de contexte";
"chatlist.context_tokens_desc" = "Définit la taille de la fenêtre de contexte utilisée pour générer le jeton suivant. (Par défaut : 2048)";
"chatlist.top_k" = "Top K";
"chatlist.top_k_desc" = "Réduit la probabilité de générer du non-sens. Une valeur plus élevée (par exemple 100) donnera des réponses plus diverses, tandis qu'une valeur plus faible (par exemple 10) sera plus conservatrice. (Par défaut : 40)";
"chatlist.top_p" = "Top P";
"chatlist.top_p_desc" = "Fonctionne avec top-k. Une valeur plus élevée (par exemple 0.95) mènera à un texte plus diversifié, tandis qu'une valeur plus faible (par exemple 0.5) générera un texte plus focalisé et conservateur. (Par défaut : 0.9)";
"chatlist.delete_success" = "Le modèle a été supprimé avec succès. Vous devrez peut-être redémarrer OllamaSpring.";
"chatlist.close" = "Fermer";
"chatlist.restart_now" = "Redémarrer maintenant";
"chatlist.download_process" = "Processus de téléchargement";
"chatlist.warning" = "Avertissement";
"chatlist.delete_confirm" = "Êtes-vous sûr de vouloir supprimer %@ ?";
"chatlist.download_confirm_title" = "Téléchargement : %@";
"chatlist.download_confirm_content" = "Cela prendra quelques minutes, continuer ?";
"chatlist.model_not_exist" = "Le modèle n'existe pas. Vous devrez peut-être redémarrer OllamaSpring.";
"chatlist.later" = "Plus tard";
"chatlist.installed" = "Installé";
"chatlist.download" = "Télécharger";

// HttpProxyConfig
"proxy.enable" = "Activer le proxy HTTP";
"proxy.disable" = "Désactiver le proxy HTTP";
"proxy.host_name" = "Nom d'hôte";
"proxy.port_number" = "Numéro de port";
"proxy.enable_auth" = "Activer l'authentification proxy";
"proxy.disable_auth" = "Désactiver l'authentification proxy";
"proxy.login" = "Connexion";
"proxy.password" = "Mot de passe";
"proxy.save" = "Enregistrer";
"proxy.cancel" = "Annuler";

// Modal
"modal.cancel" = "Annuler";
"modal.confirm" = "Confirmer";

// DeepSeek API Config
"deepseek.api_title" = "API DeepSeek";
"deepseek.enter_secret_key" = "ENTRER LA CLÉ SECRÈTE";
"deepseek.how_to_apply" = "Comment demander une clé API DeepSeek ?";
"deepseek.click_here" = "cliquez ici";
"deepseek.connection_failed" = "Échec de la connexion";
"deepseek.connection_failed_desc" = "Impossible de se connecter à l'hôte DeepSeek. Veuillez vérifier votre clé API ou la configuration du proxy HTTP.";
"deepseek.ok" = "OK";
"deepseek.description" = "DeepSeek réalise une percée significative dans la vitesse d'inférence par rapport aux modèles précédents. Il se classe en tête du classement parmi les modèles open-source et rivalise avec les modèles closed-source les plus avancés au monde.";

// Groq API Config
"groq.api_title" = "API Groq Fast";
"groq.enter_secret_key" = "ENTRER LA CLÉ SECRÈTE";
"groq.how_to_apply" = "Comment demander une clé API Groq ?";
"groq.click_here" = "cliquez ici";
"groq.description" = "Groq est une inférence IA rapide, alimentée par la technologie d'inférence IA LPU™ qui fournit une IA rapide, abordable et économe en énergie.";

// Ollama Host Config
"ollama.host_config_title" = "Configuration de l'hôte HTTP Ollama";
"ollama.connection_failed" = "Échec de la connexion";
"ollama.connection_failed_desc" = "Impossible de se connecter à l'hôte Ollama. Veuillez vérifier votre configuration et réessayer.";
"ollama.ok" = "OK";

// StatusBar
"statusbar.option_1" = "Option 1";
"statusbar.option_2" = "Option 2";
"statusbar.quit" = "Quitter";

// Messages
"messages.code_block_text" = "Texte";
"messages.share_preview" = "Partager le message OllamaSpring";
"messages.copied" = "COPIÉ";

// Menu
"menu.check_for_updates" = "Vérifier les mises à jour…";

// Ollama Library
"ollama.library.install_title" = "Installer le modèle Ollama";
"ollama.library.enter_model_name" = "Entrer le nom du modèle";
"ollama.library.what_is_model_name" = "qu'est-ce que le nom du modèle ? comme llama3:70b";
"ollama.library.click_here" = "cliquez ici";
"ollama.library.warning" = "ATTENTION ! Tous les modèles de la bibliothèque ollama ne prennent pas en charge les conversations de chat. Tout comme CodeGemma fonctionne comme un modèle fill-in-the-middle pour la complétion de code.";

// Ollama Host Config
"ollama.description" = "Configurez l'hôte et le port HTTP Ollama. Par défaut, l'hôte est défini sur 127.0.0.1 et le port sur 11434 dans votre environnement local. Vous ne pouvez vous connecter qu'aux hôtes distants qui ne nécessitent pas d'authentification.";

// Quick Completion
"quick.prompt" = "Prompt";
"quick.text" = "Texte";
"quick.waiting" = "En attente...";
"quick.copied" = "COPIÉ";
"quick.empty_input" = "Dites-moi simplement quelle est votre question.";
"quick.no_model" = "Aucun modèle trouvé. Vous devrez peut-être d'abord télécharger un modèle et redémarrer OllamaSpring.";
"quick.ollama_not_available" = "Vous devez d'abord démarrer Ollama ou l'installer.";
"quick.shortcut_hint" = "Vous pouvez ouvrir la complétion rapide avec le raccourci cmd + shift + h";

// Right Top Bar
"righttopbar.api_host" = "Hôte API";
"righttopbar.confirm" = "Confirmer";
"righttopbar.deepseek_api_key_config" = "Configuration de la clé API DeepSeek";
"righttopbar.groq_api_key_config" = "Configuration de la clé API Groq";
"righttopbar.groq_no_streaming" = "Groq ne prend pas en charge la sortie en streaming";
"righttopbar.library" = "Marché";
"righttopbar.no_models_found" = "Aucun modèle trouvé";
"righttopbar.notice" = "Avis";
"righttopbar.ollama_http_host_config" = "Configuration de l'hôte HTTP Ollama";
"righttopbar.response_language" = "Langue de réponse";
"righttopbar.streaming" = "Streaming";
"righttopbar.unknown_model" = "Modèle inconnu";
"messages.assistant" = "Assistant";


