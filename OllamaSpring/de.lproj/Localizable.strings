//
//  Localizable.strings
//  OllamaSpring
//
//  Created by NeilStudio on 2025/6/17.
//

"Welcome to OllamaSpring" = "Willkommen bei OllamaSpring";
"welcome.help_today" = "Wie kann ich Ihnen heute helfen?";
"welcome.no_model_message" = "Entschuldigung, Sie müssen zuerst ein Ollama-Modell herunterladen. Sie finden einen Download-Button unten links. Viel Spaß!";
"welcome.description" = "OllamaSpring ist ein umfassender Mac-Client zur Verwaltung der verschiedenen Modelle der Ollama-Community und zur Erstellung von konversationellen KI-Erlebnissen.";

// SendMsgPanel
"sendmsg.revoke" = "Widerrufen";
"sendmsg.voice_not_available" = "Sprach-zu-Text ist derzeit nicht verfügbar";
"sendmsg.deepseek_upload_coming" = "Datei-Upload für DeepSeek kommt bald";
"sendmsg.select_model_first" = "Sie müssen zuerst ein Modell in der oberen Leiste auswählen oder ein Modell herunterladen";
"sendmsg.create_conversation_first" = "Sie müssen zuerst ein neues Gespräch in der oberen linken Leiste erstellen.";

// MainPanel
"main.welcome" = "Willkommen bei OllamaSpring";
"main.start_without_ollama" = "Ohne Ollama starten";
"main.ollama_not_available" = "Ollama API-Service ist auf Ihrem Mac nicht verfügbar. Wenn Sie Ollama-Modelle lokal auf Ihrem Mac ausführen möchten, folgen Sie diesen Schritten, um Ollama zu installieren und einzurichten. Wenn Sie Ollama API-Service auf einem bestimmten Host hosten, geben Sie einfach Ihren eigenen Ollama-Host unten ein.";
"main.step1_install" = "Schritt 1: Ollama installieren";
"main.step2_refresh" = "Schritt 2: Aktualisieren";
"main.enter_ollama_host" = "Geben Sie Ihren eigenen Ollama-Host ein";

// MessagesPanel
"messages.assistant" = "Assistent";
"messages.text" = "Text";
"messages.waiting" = "Warten...";

// ChatListPanel
"chatlist.conversation" = "Gespräch";
"chatlist.model" = "Modell";
"chatlist.download_first" = "Sie sollten zuerst ein Modell herunterladen und ein bevorzugtes auswählen, bevor Sie einen neuen Chat erstellen";
"chatlist.remove" = "Entfernen";
"chatlist.downloads" = "Downloads";
"chatlist.reset_all" = "Alles zurücksetzen";
"chatlist.temperature" = "Temperatur";
"chatlist.temperature_desc" = "Die Temperatur des Modells. Eine Erhöhung der Temperatur macht die Antworten des Modells kreativer. (Standard: 0.8)";
"chatlist.seed" = "Seed";
"chatlist.seed_desc" = "Setzt den Zufallszahlen-Seed für die Generierung. Das Setzen auf eine bestimmte Zahl bewirkt, dass das Modell für denselben Prompt denselben Text generiert. (Standard: 0)";
"chatlist.context_tokens" = "Kontext-Tokens";
"chatlist.context_tokens_desc" = "Setzt die Größe des Kontextfensters, das zur Generierung des nächsten Tokens verwendet wird. (Standard: 2048)";
"chatlist.top_k" = "Top K";
"chatlist.top_k_desc" = "Reduziert die Wahrscheinlichkeit, Unsinn zu generieren. Ein höherer Wert (z.B. 100) gibt vielfältigere Antworten, während ein niedrigerer Wert (z.B. 10) konservativer ist. (Standard: 40)";
"chatlist.top_p" = "Top P";
"chatlist.top_p_desc" = "Funktioniert zusammen mit top-k. Ein höherer Wert (z.B. 0.95) führt zu vielfältigerem Text, während ein niedrigerer Wert (z.B. 0.5) fokussierteren und konservativeren Text generiert. (Standard: 0.9)";
"chatlist.delete_success" = "Das Modell wurde erfolgreich gelöscht. Sie sollten OllamaSpring neu starten.";
"chatlist.close" = "Schließen";
"chatlist.restart_now" = "Jetzt neu starten";
"chatlist.download_process" = "Download-Fortschritt";
"chatlist.warning" = "Warnung";
"chatlist.delete_confirm" = "Sind Sie sicher, dass Sie %@ löschen möchten?";
"chatlist.download_confirm_title" = "Download: %@";
"chatlist.download_confirm_content" = "Dies wird einige Minuten dauern, fortfahren?";
"chatlist.model_not_exist" = "Das Modell existiert nicht. Sie sollten OllamaSpring neu starten.";
"chatlist.later" = "Später";
"chatlist.installed" = "Installiert";
"chatlist.download" = "Download";

// HttpProxyConfig
"proxy.enable" = "HTTP-Proxy aktivieren";
"proxy.disable" = "HTTP-Proxy deaktivieren";
"proxy.host_name" = "Host-Name";
"proxy.port_number" = "Port-Nummer";
"proxy.enable_auth" = "Proxy-Authentifizierung aktivieren";
"proxy.disable_auth" = "Proxy-Authentifizierung deaktivieren";
"proxy.login" = "Anmeldung";
"proxy.password" = "Passwort";
"proxy.save" = "Speichern";
"proxy.cancel" = "Abbrechen";

// Modal
"modal.cancel" = "Abbrechen";
"modal.confirm" = "Bestätigen";

// DeepSeek API Config
"deepseek.api_title" = "DeepSeek API";
"deepseek.enter_secret_key" = "GEHEIMEN SCHLÜSSEL EINGEBEN";
"deepseek.how_to_apply" = "Wie beantrage ich einen DeepSeek API-Schlüssel?";
"deepseek.click_here" = "hier klicken";
"deepseek.connection_failed" = "Verbindung fehlgeschlagen";
"deepseek.connection_failed_desc" = "Verbindung zum DeepSeek-Host fehlgeschlagen. Bitte überprüfen Sie Ihren API-Schlüssel oder die HTTP-Proxy-Konfiguration.";
"deepseek.ok" = "OK";
"deepseek.description" = "DeepSeek erzielt einen bedeutenden Durchbruch in der Inferenzgeschwindigkeit gegenüber früheren Modellen. Es führt die Rangliste unter Open-Source-Modellen an und konkurriert mit den fortschrittlichsten Closed-Source-Modellen weltweit.";

// Groq API Config
"groq.api_title" = "Groq Fast API";
"groq.enter_secret_key" = "GEHEIMEN SCHLÜSSEL EINGEBEN";
"groq.how_to_apply" = "Wie beantrage ich einen Groq API-Schlüssel?";
"groq.click_here" = "hier klicken";
"groq.description" = "Groq ist eine schnelle KI-Inferenz, angetrieben von LPU™ AI-Inferenztechnologie, die schnelle, erschwingliche und energieeffiziente KI liefert.";

// Ollama Host Config
"ollama.host_config_title" = "Ollama HTTP-Host-Konfiguration";
"ollama.connection_failed" = "Verbindung fehlgeschlagen";
"ollama.connection_failed_desc" = "Verbindung zum Ollama-Host fehlgeschlagen. Bitte überprüfen Sie Ihre Konfiguration und versuchen Sie es erneut.";
"ollama.ok" = "OK";

// StatusBar
"statusbar.option_1" = "Option 1";
"statusbar.option_2" = "Option 2";
"statusbar.quit" = "Beenden";

// Messages
"messages.code_block_text" = "Text";
"messages.share_preview" = "OllamaSpring-Nachricht teilen";
"messages.copied" = "KOPIERT";

// Menu
"menu.check_for_updates" = "Nach Updates suchen…";

// Ollama Library
"ollama.library.install_title" = "Ollama-Modell installieren";
"ollama.library.enter_model_name" = "Modellname eingeben";
"ollama.library.what_is_model_name" = "Was ist der Modellname? wie llama3:70b";
"ollama.library.click_here" = "hier klicken";
"ollama.library.warning" = "WARNUNG! Nicht alle Ollama-Bibliotheksmodelle unterstützen Chat-Gespräche. Genau wie CodeGemma als Fill-in-the-Middle-Modell für Code-Vervollständigung funktioniert.";

// Ollama Host Config
"ollama.description" = "Konfigurieren Sie den Ollama HTTP-Host und Port. Standardmäßig ist der Host auf 127.0.0.1 und der Port auf 11434 in Ihrer lokalen Umgebung eingestellt. Sie können nur eine Verbindung zu Remote-Hosts herstellen, die keine Authentifizierung erfordern.";

// Quick Completion
"quick.prompt" = "Prompt";
"quick.text" = "Text";
"quick.waiting" = "Warten...";
"quick.copied" = "KOPIERT";
"quick.empty_input" = "Sagen Sie mir einfach, was Ihre Frage ist.";
"quick.no_model" = "Kein Modell gefunden. Sie müssen möglicherweise zuerst ein Modell herunterladen und OllamaSpring neu starten.";
"quick.ollama_not_available" = "Sie sollten zuerst Ollama starten oder installieren.";
"quick.shortcut_hint" = "Sie können die schnelle Vervollständigung mit der Tastenkombination cmd + shift + h öffnen";

// Right Top Bar
"righttopbar.api_host" = "API-Host";
"righttopbar.confirm" = "Bestätigen";
"righttopbar.deepseek_api_key_config" = "DeepSeek API-Schlüssel-Konfiguration";
"righttopbar.groq_api_key_config" = "Groq API-Schlüssel-Konfiguration";
"righttopbar.groq_no_streaming" = "Groq unterstützt kein Streaming-Output";
"righttopbar.library" = "Markt";
"righttopbar.no_models_found" = "Keine Modelle gefunden";
"righttopbar.notice" = "Hinweis";
"righttopbar.ollama_http_host_config" = "Ollama HTTP-Host-Konfiguration";
"righttopbar.response_language" = "Antwortsprache";
"righttopbar.streaming" = "Streaming";
"righttopbar.unknown_model" = "Unbekanntes Modell";
"messages.assistant" = "Assistent";


