//
//  Localizable.strings
//  OllamaSpring
//
//  Created by NeilStudio on 2025/6/17.
//

"Welcome to OllamaSpring" = "Bienvenido a OllamaSpring";
"welcome.help_today" = "¿Cómo puedo ayudarte hoy?";
"welcome.no_model_message" = "Lo siento, primero necesitas descargar un modelo de Ollama. Puedes encontrar un botón de descarga en la esquina inferior izquierda. ¡Disfruta!";
"welcome.description" = "OllamaSpring es un cliente Mac completo para gestionar los diversos modelos ofrecidos por la comunidad ollama y crear experiencias de IA conversacional.";

// SendMsgPanel
"sendmsg.revoke" = "Revocar";
"sendmsg.voice_not_available" = "La conversión de voz a texto no está disponible actualmente";
"sendmsg.deepseek_upload_coming" = "La carga de archivos para DeepSeek llegará pronto";
"sendmsg.select_model_first" = "Primero necesitas seleccionar un modelo en la barra superior o descargar un modelo";
"sendmsg.create_conversation_first" = "Primero necesitas crear una nueva conversación en la barra superior izquierda.";

// MainPanel
"main.welcome" = "Bienvenido a OllamaSpring";
"main.start_without_ollama" = "Iniciar sin Ollama";
"main.ollama_not_available" = "El servicio API de Ollama no está disponible en tu Mac. Si quieres ejecutar modelos de Ollama localmente en tu Mac, sigue estos pasos para instalar y configurar Ollama primero. Si alojas el servicio API de Ollama en un host específico, simplemente ingresa tu propio host de Ollama abajo.";
"main.step1_install" = "Paso 1: Instalar Ollama";
"main.step2_refresh" = "Paso 2: Actualizar";
"main.enter_ollama_host" = "Ingresa tu propio host de Ollama";

// MessagesPanel
"messages.assistant" = "Asistente";
"messages.text" = "Texto";
"messages.waiting" = "Esperando...";

// ChatListPanel
"chatlist.conversation" = "Conversación";
"chatlist.model" = "Modelo";
"chatlist.download_first" = "Primero debes descargar un modelo y seleccionar uno preferido antes de crear un nuevo chat";
"chatlist.remove" = "Eliminar";
"chatlist.downloads" = "Descargas";
"chatlist.reset_all" = "Restablecer todo";
"chatlist.temperature" = "Temperatura";
"chatlist.temperature_desc" = "La temperatura del modelo. Aumentar la temperatura hará que las respuestas del modelo sean más creativas. (Predeterminado: 0.8)";
"chatlist.seed" = "Semilla";
"chatlist.seed_desc" = "Establece la semilla de números aleatorios para usar en la generación. Establecer esto en un número específico hará que el modelo genere el mismo texto para el mismo prompt. (Predeterminado: 0)";
"chatlist.context_tokens" = "Tokens de contexto";
"chatlist.context_tokens_desc" = "Establece el tamaño de la ventana de contexto utilizada para generar el siguiente token. (Predeterminado: 2048)";
"chatlist.top_k" = "Top K";
"chatlist.top_k_desc" = "Reduce la probabilidad de generar sin sentido. Un valor más alto (por ejemplo 100) dará respuestas más diversas, mientras que un valor más bajo (por ejemplo 10) será más conservador. (Predeterminado: 40)";
"chatlist.top_p" = "Top P";
"chatlist.top_p_desc" = "Funciona junto con top-k. Un valor más alto (por ejemplo 0.95) llevará a texto más diverso, mientras que un valor más bajo (por ejemplo 0.5) generará texto más enfocado y conservador. (Predeterminado: 0.9)";
"chatlist.delete_success" = "El modelo ha sido eliminado exitosamente. Es posible que necesites reiniciar OllamaSpring.";
"chatlist.close" = "Cerrar";
"chatlist.restart_now" = "Reiniciar ahora";
"chatlist.download_process" = "Proceso de descarga";
"chatlist.warning" = "Advertencia";
"chatlist.delete_confirm" = "¿Estás seguro de que quieres eliminar %@?";
"chatlist.download_confirm_title" = "Descarga: %@";
"chatlist.download_confirm_content" = "Esto tomará unos minutos, ¿continuar?";
"chatlist.model_not_exist" = "El modelo no existe. Es posible que necesites reiniciar OllamaSpring.";
"chatlist.later" = "Más tarde";
"chatlist.installed" = "Instalado";
"chatlist.download" = "Descargar";

// HttpProxyConfig
"proxy.enable" = "Habilitar proxy HTTP";
"proxy.disable" = "Deshabilitar proxy HTTP";
"proxy.host_name" = "Nombre del host";
"proxy.port_number" = "Número de puerto";
"proxy.enable_auth" = "Habilitar autenticación de proxy";
"proxy.disable_auth" = "Deshabilitar autenticación de proxy";
"proxy.login" = "Inicio de sesión";
"proxy.password" = "Contraseña";
"proxy.save" = "Guardar";
"proxy.cancel" = "Cancelar";

// Modal
"modal.cancel" = "Cancelar";
"modal.confirm" = "Confirmar";

// DeepSeek API Config
"deepseek.api_title" = "API DeepSeek";
"deepseek.enter_secret_key" = "INGRESAR CLAVE SECRETA";
"deepseek.how_to_apply" = "¿Cómo solicitar una clave API de DeepSeek?";
"deepseek.click_here" = "haz clic aquí";
"deepseek.connection_failed" = "Error de conexión";
"deepseek.connection_failed_desc" = "No se pudo conectar al host de DeepSeek. Por favor verifica tu clave API o la configuración del proxy HTTP.";
"deepseek.ok" = "OK";
"deepseek.description" = "DeepSeek logra un avance significativo en la velocidad de inferencia sobre modelos anteriores. Encabeza la tabla de clasificación entre modelos de código abierto y compite con los modelos de código cerrado más avanzados del mundo.";

// Groq API Config
"groq.api_title" = "API Groq Fast";
"groq.enter_secret_key" = "INGRESAR CLAVE SECRETA";
"groq.how_to_apply" = "¿Cómo solicitar una clave API de Groq?";
"groq.click_here" = "haz clic aquí";
"groq.description" = "Groq es una inferencia de IA rápida, impulsada por la tecnología de inferencia de IA LPU™ que proporciona IA rápida, asequible y energéticamente eficiente.";

// Ollama Host Config
"ollama.host_config_title" = "Configuración del host HTTP de Ollama";
"ollama.connection_failed" = "Error de conexión";
"ollama.connection_failed_desc" = "No se pudo conectar al host de Ollama. Por favor verifica tu configuración e inténtalo de nuevo.";
"ollama.ok" = "OK";

// StatusBar
"statusbar.option_1" = "Opción 1";
"statusbar.option_2" = "Opción 2";
"statusbar.quit" = "Salir";

// Messages
"messages.code_block_text" = "Texto";
"messages.share_preview" = "Compartir mensaje de OllamaSpring";
"messages.copied" = "COPIADO";

// Menu
"menu.check_for_updates" = "Buscar actualizaciones…";

// Ollama Library
"ollama.library.install_title" = "Instalar modelo de Ollama";
"ollama.library.enter_model_name" = "Ingresar nombre del modelo";
"ollama.library.what_is_model_name" = "¿qué es el nombre del modelo? como llama3:70b";
"ollama.library.click_here" = "haz clic aquí";
"ollama.library.warning" = "¡ADVERTENCIA! No todos los modelos de la biblioteca ollama admiten conversaciones de chat. Al igual que CodeGemma funciona como un modelo fill-in-the-middle para la completación de código.";

// Ollama Host Config
"ollama.description" = "Configura el host y puerto HTTP de Ollama. Por defecto, el host está configurado en 127.0.0.1 y el puerto en 11434 en tu entorno local. Solo puedes conectarte a hosts remotos que no requieran autenticación.";

// Quick Completion
"quick.prompt" = "Prompt";
"quick.text" = "Texto";
"quick.waiting" = "Esperando...";
"quick.copied" = "COPIADO";
"quick.empty_input" = "Solo dime cuál es tu pregunta.";
"quick.no_model" = "No se encontró ningún modelo. Es posible que necesites descargar un modelo y reiniciar OllamaSpring primero.";
"quick.ollama_not_available" = "Debes iniciar Ollama o instalarlo primero.";
"quick.shortcut_hint" = "Puedes abrir la completación rápida con el atajo cmd + shift + h";

// Right Top Bar
"righttopbar.api_host" = "Host API";
"righttopbar.confirm" = "Confirmar";
"righttopbar.deepseek_api_key_config" = "Configuración de clave API de DeepSeek";
"righttopbar.groq_api_key_config" = "Configuración de clave API de Groq";
"righttopbar.groq_no_streaming" = "Groq no admite salida en streaming";
"righttopbar.library" = "Mercado";
"righttopbar.no_models_found" = "No se encontraron modelos";
"righttopbar.notice" = "Aviso";
"righttopbar.ollama_http_host_config" = "Configuración del host HTTP de Ollama";
"righttopbar.response_language" = "Idioma de respuesta";
"righttopbar.streaming" = "Streaming";
"righttopbar.unknown_model" = "Modelo desconocido";
"messages.assistant" = "Asistente";


