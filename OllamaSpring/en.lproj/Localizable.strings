//
//  Localizable.strings
//  OllamaSpring
//
//  Created by NeilStudio on 2025/6/17.
//

"Welcome to OllamaSpring" = "Welcome to OllamaSpring";
"welcome.help_today" = "How can I help you today?";
"welcome.no_model_message" = "Oops, you need to download a Ollama model first. You can find a 'Downloads' button at the bottom left. Enjoy!";
"welcome.description" = "OllamaSpring is a comprehensive Mac client for managing the various models offered by the ollama community, and for creating conversational AI experiences.";

// SendMsgPanel
"sendmsg.revoke" = "Revoke";
"sendmsg.voice_not_available" = "Voice-to-text is not available currently";
"sendmsg.deepseek_upload_coming" = "File upload for DeepSeek is coming soon";
"sendmsg.select_model_first" = "You need select a model on top bar first or download a model first";
"sendmsg.create_conversation_first" = "You need create a new conversation on left top bar first.";

// MainPanel
"main.welcome" = "Welcome To OllamaSpring";
"main.start_without_ollama" = "Start Without Ollama";
"main.ollama_not_available" = "Ollama API service is not available on your Mac. If you want to run Ollama models locally on your Mac, follow these steps to install and set up Ollama first. If you host Ollama api service on specific host, you should just enter your own Ollama host below.";
"main.step1_install" = "Step 1: Install Ollama";
"main.step2_refresh" = "Step 2: Refresh";
"main.enter_ollama_host" = "Enter your own Ollama host";

// MessagesPanel
"messages.assistant" = "Assistant";
"messages.text" = "Text";
"messages.waiting" = "waiting ...";

// ChatListPanel
"chatlist.conversation" = "Conversation";
"chatlist.download_first" = "You should download a model first and select a preffered one before creating a new chat";
"chatlist.remove" = "Remove";
"chatlist.downloads" = "Downloads";
"chatlist.reset_all" = "Reset All";
"chatlist.temperature" = "Temperature";
"chatlist.temperature_desc" = "The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8)";
"chatlist.seed" = "Seed";
"chatlist.seed_desc" = "Sets the random number seed to use for generation. Setting this to a specific number will make the model generate the same text for the same prompt. (Default: 0)";
"chatlist.context_tokens" = "Context Tokens";
"chatlist.context_tokens_desc" = "Sets the size of the context window used to generate the next token. (Default: 2048)";
"chatlist.top_k" = "Top K";
"chatlist.top_k_desc" = "Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)";
"chatlist.top_p" = "Top P";
"chatlist.top_p_desc" = "Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)";
"chatlist.delete_success" = "The model have been deleted successfully. You may want to restat OllamaSpring.";
"chatlist.close" = "Close";
"chatlist.restart_now" = "Restart Now";
"chatlist.download_process" = "Download Process";
"chatlist.warning" = "Warning";
"chatlist.delete_confirm" = "Are you sure to delete %@?";
"chatlist.download_confirm_title" = "Download: %@";
"chatlist.download_confirm_content" = "This will take a few minutes, continue?";
"chatlist.model_not_exist" = "The model does not exist. You may want to restart OllamaSpring.";
"chatlist.later" = "Later";
"chatlist.installed" = "Installed";
"chatlist.download" = "Download";
"chatlist.model" = "Model";

// HttpProxyConfig
"proxy.enable" = "Enable Http Proxy";
"proxy.disable" = "Disable Http Proxy";
"proxy.host_name" = "Host Name";
"proxy.port_number" = "Port Number";
"proxy.enable_auth" = "Enable Proxy Auth";
"proxy.disable_auth" = "Disable Proxy Auth";
"proxy.login" = "Login";
"proxy.password" = "Password";
"proxy.save" = "Save";
"proxy.cancel" = "Cancel";

// Modal
"modal.cancel" = "Cancel";
"modal.confirm" = "Confirm";

// DeepSeek API Config
"deepseek.api_title" = "DeepSeek API";
"deepseek.enter_secret_key" = "ENTER SECRET KEY";
"deepseek.how_to_apply" = "How to apply a DeepSeek API key?";
"deepseek.click_here" = "click here";
"deepseek.connection_failed" = "Connection Failed";
"deepseek.connection_failed_desc" = "Failed to connect to DeepSeek host. Please verify your apiKey or HTTP Proxy Config.";
"deepseek.ok" = "OK";
"deepseek.description" = "DeepSeek achieves a significant breakthrough in inference speed over previous models. It tops the leaderboard among open-source models and rivals the most advanced closed-source models globally.";

// Groq API Config
"groq.api_title" = "Groq Fast API";
"groq.enter_secret_key" = "ENTER SECRET KEY";
"groq.how_to_apply" = "How to apply a Groq API key?";
"groq.click_here" = "click here";
"groq.description" = "Groq is a fast AI inference, powered by LPU™ AI inference technology which delivers fast, affordable, and energy efficient AI.";

// Ollama Host Config
"ollama.host_config_title" = "Ollama HTTP Host Configuration";
"ollama.connection_failed" = "Connection Failed";
"ollama.connection_failed_desc" = "Failed to connect to Ollama host. Please check your configuration and try again.";
"ollama.ok" = "OK";
"ollama.description" = "Configure the Ollama HTTP host and port. By default, the host is set to 127.0.0.1 and the port to 11434 in your local environment. You may only connect to remote hosts that do not require authentication.";

// Ollama Library
"ollama.library.install_title" = "Install Ollama Model";
"ollama.library.enter_model_name" = "Enter Model Name";
"ollama.library.what_is_model_name" = "what is model name? like llama3:70b";
"ollama.library.click_here" = "click here";
"ollama.library.warning" = "WARNING! Not all ollama library models support chat conversations. Just like CodeGemma works as a fill-in-the-middle model for code completion.";

// Menu
"menu.check_for_updates" = "Check for Updates…";

// Right Top Bar
"righttopbar.library" = "Market";
"righttopbar.response_language" = "Response Language";
"righttopbar.api_host" = "API Host";
"righttopbar.streaming" = "Streaming";
"righttopbar.no_models_found" = "No models found";
"righttopbar.unknown_model" = "Unknown Model";
"righttopbar.groq_api_key_config" = "Groq API Key Config";
"righttopbar.ollama_http_host_config" = "Ollama HTTP Host Config";
"righttopbar.deepseek_api_key_config" = "DeepSeek API Key Config";
"righttopbar.notice" = "Notice";
"righttopbar.groq_no_streaming" = "Groq not support Streaming Output";
"righttopbar.confirm" = "Confirm";

// Status Bar
"statusbar.option_1" = "Option 1";
"statusbar.option_2" = "Option 2";
"statusbar.quit" = "Quit";

// QuickCompletion
"quick.prompt" = "Prompt";
"quick.shortcut_hint" = "You can open quick completion by shortcut cmd + shift + h";
"quick.ollama_not_available" = "You should start Ollama or install it first.";
"quick.no_model" = "No model found. You may need to download a model and restart OllamaSpring first.";
"quick.empty_input" = "Just tell me what is your question.";
"quick.waiting" = "waiting ...";
"quick.text" = "Text";
"quick.copied" = "COPIED";

// MessagesRowView
"messages.assistant" = "Assistant";
"messages.copied" = "COPIED";
"messages.code_block_text" = "Text";
"messages.share_preview" = "Share OllamaSpring Message";


